<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>分类任务</title>
      <link href="/2026/01/03/%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1/"/>
      <url>/2026/01/03/%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1/</url>
      
        <content type="html"><![CDATA[<h1 id="理论问题环节"><a href="#理论问题环节" class="headerlink" title="理论问题环节"></a>理论问题环节</h1><h2 id="为什么我们需要卷积神经网络？"><a href="#为什么我们需要卷积神经网络？" class="headerlink" title="为什么我们需要卷积神经网络？"></a>为什么我们需要卷积神经网络？</h2><p>因为全连接神经网络的参数量过大，容易导致模型过拟合</p><h2 id="输入必须是3-256-256吗"><a href="#输入必须是3-256-256吗" class="headerlink" title="输入必须是3 * 256 * 256吗"></a>输入必须是3 * 256 * 256吗</h2><p>不一定，这是一种<strong>非常经典且常用</strong>的输入尺寸但它并非 CNN 的强制要求，这个尺寸主要来源于早期的经典预训练模型，如<strong>AlexNet、VGG、ResNet 等经典预训练模型</strong>它们在训练时默认采用了 224×224 的图像分辨率，而 3 代表 RGB 彩色图像的 3 个通道（通道数 × 高度 × 宽度，即 C×H×W）。</p><h2 id="每次卷积特征图的尺寸都变小了，如何保持尺度不变呢？"><a href="#每次卷积特征图的尺寸都变小了，如何保持尺度不变呢？" class="headerlink" title="每次卷积特征图的尺寸都变小了，如何保持尺度不变呢？"></a>每次卷积特征图的尺寸都变小了，如何保持尺度不变呢？</h2><p>零填充</p><h2 id="更大的卷积核和更多的卷积核层数意味着什么？"><a href="#更大的卷积核和更多的卷积核层数意味着什么？" class="headerlink" title="更大的卷积核和更多的卷积核层数意味着什么？"></a>更大的卷积核和更多的卷积核层数意味着什么？</h2><p>更大的卷积核意味着更大的感受野，</p><p>更多的卷积核层数，就像<strong>给模型升级成 “多级侦探”</strong>，能从表面细节挖到深层本质，处理更复杂的任务；但层数太多会出现 **“信号断联”<strong>的问题（梯度消失 &#x2F; 爆炸）</strong></p><ul><li><strong>梯度消失</strong>：指令要经过很多层侦探（层数多），每传一层就被削弱一点，传到最前面的新手侦探时，指令已经弱到听不见了（梯度趋近于 0），新手侦探没法纠正自己的错误，模型学不会。</li><li><strong>梯度爆炸</strong>：反过来，指令每传一层就被放大一点，传到前面时已经变成 “噪音”（梯度过大），新手侦探收到混乱的指令，越改越错，模型训练崩溃。</li></ul><h2 id="卷积核和特征图是什么？"><a href="#卷积核和特征图是什么？" class="headerlink" title="卷积核和特征图是什么？"></a>卷积核和特征图是什么？</h2><p><img src="/images/2.png" alt="image-20260103112342091"></p><h2 id="特征图和卷积核的深度有什么关系？"><a href="#特征图和卷积核的深度有什么关系？" class="headerlink" title="特征图和卷积核的深度有什么关系？"></a>特征图和卷积核的深度有什么关系？</h2><p><img src="/images/3.png" alt="image-20260103112828611"></p><h2 id="一个卷积层是什么？卷积核的参数量如何计算？"><a href="#一个卷积层是什么？卷积核的参数量如何计算？" class="headerlink" title="一个卷积层是什么？卷积核的参数量如何计算？"></a>一个卷积层是什么？卷积核的参数量如何计算？</h2><p>整个模型唯一能控制的量就是卷积层</p><p><img src="/images/4.png" alt="image-20260103113301126"></p><h2 id="padding如何填充？4-4经过一次零填充变成什么样子？"><a href="#padding如何填充？4-4经过一次零填充变成什么样子？" class="headerlink" title="padding如何填充？4 * 4经过一次零填充变成什么样子？"></a>padding如何填充？4 * 4经过一次零填充变成什么样子？</h2><p>6 * 6</p><h2 id="但padding之后特征图大小一直保持不变，仍然需要大量参数怎么办？怎么样才能降低参数量"><a href="#但padding之后特征图大小一直保持不变，仍然需要大量参数怎么办？怎么样才能降低参数量" class="headerlink" title="但padding之后特征图大小一直保持不变，仍然需要大量参数怎么办？怎么样才能降低参数量"></a>但padding之后特征图大小一直保持不变，仍然需要大量参数怎么办？怎么样才能降低参数量</h2><p>池化可以把长、宽对半砍</p><h2 id="卷积尺寸计算"><a href="#卷积尺寸计算" class="headerlink" title="卷积尺寸计算"></a>卷积尺寸计算</h2><p>$$<br>[<br>H_{out} &#x3D; \left\lfloor \frac{H_{in} + 2 \times \text{padding} - \text{kernel_size}}{\text{stride}} \right\rfloor + 1<br>]<br>$$</p><p><img src="/images/5.png" alt="image-20260103115146579"></p><h2 id="减少特征图尺寸的方法"><a href="#减少特征图尺寸的方法" class="headerlink" title="减少特征图尺寸的方法"></a>减少特征图尺寸的方法</h2><p>池化和增加步长都可以减少特征图尺寸**（注意不是减少卷积核的参数）**，但池化一般比增加步长更常用。</p><h2 id="所以如何改变图片的参数大小呢？"><a href="#所以如何改变图片的参数大小呢？" class="headerlink" title="所以如何改变图片的参数大小呢？"></a>所以如何改变图片的参数大小呢？</h2><p><img src="/images/6.png" alt="image-20260103120052449"></p><h2 id="不要忘记初心，我们一开始是为了分出一个类别"><a href="#不要忘记初心，我们一开始是为了分出一个类别" class="headerlink" title="不要忘记初心，我们一开始是为了分出一个类别"></a>不要忘记初心，我们一开始是为了分出一个类别</h2><p><img src="/images/7.png" alt="image-20260103120159167"></p><h2 id="loss如何衡量？"><a href="#loss如何衡量？" class="headerlink" title="loss如何衡量？"></a>loss如何衡量？</h2><p>使用softmax，把计算出来的、差距不大的数值，转变成最终的、差距很大的概率</p><p><img src="/images/8.png" alt="image-20260103122636467"></p><h2 id="经典神经网络AlexNet做了什么改进？"><a href="#经典神经网络AlexNet做了什么改进？" class="headerlink" title="经典神经网络AlexNet做了什么改进？"></a>经典神经网络AlexNet做了什么改进？</h2><p>relu、dropout、池化、归一化</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>with的用法</title>
      <link href="/2026/01/01/with%E7%9A%84%E7%94%A8%E6%B3%95/"/>
      <url>/2026/01/01/with%E7%9A%84%E7%94%A8%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h1 id="用法一：文件"><a href="#用法一：文件" class="headerlink" title="用法一：文件"></a>用法一：文件</h1><h2 id="如果不使用with，我们应该如何打开一个文件？"><a href="#如果不使用with，我们应该如何打开一个文件？" class="headerlink" title="如果不使用with，我们应该如何打开一个文件？"></a>如果不使用with，我们应该如何打开一个文件？</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="comment"># 1. [进入]</span></span><br><span class="line">    f = <span class="built_in">open</span>(<span class="string">&#x27;a.txt&#x27;</span>, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">    <span class="comment"># 2. [执行]</span></span><br><span class="line">    <span class="built_in">print</span>(f.read())</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="keyword">if</span> f:</span><br><span class="line">        <span class="comment"># 3. [退出]</span></span><br><span class="line">        f.close()</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>with在这里就起到一个封装try和finally的作用，finally的作用就是如果文件在io过程中产生了错误，也能正常执行f.close()安全关闭文件</p><h2 id="写成with语句"><a href="#写成with语句" class="headerlink" title="写成with语句"></a>写成with语句</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;a.txt&quot;</span>, <span class="string">&quot;r&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="built_in">print</span>(f.read())</span><br></pre></td></tr></table></figure><p>即把open(“a.txt”, “r”, encoding&#x3D;”utf-8”)的结果命名为f，再执行print(f.read())的语句块。</p><h1 id="用法二：pytorch梯度管理"><a href="#用法二：pytorch梯度管理" class="headerlink" title="用法二：pytorch梯度管理"></a>用法二：pytorch梯度管理</h1><p>with语句可以临时切换计算环境状态并自动恢复初始状态。<strong>由于在验证和测试的过程中都不需要积累梯度</strong>，模型切换到eval模式使用torch.no_grad()，说明临时切换到了禁用张量的自动梯度计算模式，在计算结束后会恢复到计算梯度的状态</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">model.train()</span><br><span class="line">      train_loss = <span class="number">0.0</span></span><br><span class="line">      val_loss = <span class="number">0.0</span></span><br><span class="line">      <span class="keyword">for</span> data <span class="keyword">in</span> trainloader:</span><br><span class="line">          optimizer.zero_grad()</span><br><span class="line">          x, target = data[<span class="number">0</span>].to(device),data[<span class="number">1</span>].to(device)</span><br><span class="line">          pred = model(x)</span><br><span class="line">          bat_loss = loss(pred,target,model)</span><br><span class="line">          optimizer.step()</span><br><span class="line">          train_loss += bat_loss.detach().cpu().item()</span><br><span class="line">      plt_train_loss.append(train_loss/trainloader.dataset.__len__())</span><br><span class="line"></span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">      </span><br><span class="line">      <span class="comment"># 切换到验证模式</span></span><br><span class="line">      model.<span class="built_in">eval</span>()</span><br><span class="line">      <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">          <span class="keyword">for</span> data <span class="keyword">in</span> valloader:</span><br><span class="line">              val_x, val_target = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br><span class="line">              val_pred = model(val_x)</span><br><span class="line">              val_bat_loss = loss(val_pred,val_target,model)</span><br><span class="line">              val_loss += val_bat_loss.detach().cpu().item()</span><br><span class="line">      <span class="keyword">if</span> val_loss &lt; min_val_loss:</span><br><span class="line">          torch.save(model, save_)</span><br><span class="line">      plt_val_loss.append(val_loss/valloader.dataset.__len__())</span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="总结："><a href="#总结：" class="headerlink" title="总结："></a>总结：</h1><p>with 语句核心：Python 上下文管理器语法糖，自动初始化 + 自动收尾，异常安全。</p><p>两大核心用法：</p><p>文件操作：with open(…) as f<br>自动打开 &#x2F; 关闭文件，避免句柄泄露，替代繁琐的 try…finally。</p><p>PyTorch 梯度管理：with torch.no_grad()<br>验证 &#x2F; 测试阶段临时禁用梯度计算，自动恢复状态，需与 model.eval() 搭配，节省资源 + 提速。</p>]]></content>
      
      
      <categories>
          
          <category> python语法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> with语句 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习项目实战第三节</title>
      <link href="/2025/12/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%89%E8%8A%82/"/>
      <url>/2025/12/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98%E7%AC%AC%E4%B8%89%E8%8A%82/</url>
      
        <content type="html"><![CDATA[<p>首先我们要新建一个类，一个类里面要有三部分</p><h1 id="第一部分：初始化"><a href="#第一部分：初始化" class="headerlink" title="第一部分：初始化"></a>第一部分：初始化</h1><h2 id="第一，打开文件"><a href="#第一，打开文件" class="headerlink" title="第一，打开文件"></a>第一，打开文件</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">____________</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_train,<span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        csv_data = <span class="built_in">list</span>(csv.reader(file_train))</span><br><span class="line">        column = csv_data[<span class="number">0</span>]</span><br><span class="line">        x = csv_data[<span class="number">1</span>:, <span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">        y = csv_data[<span class="number">1</span>:, -<span class="number">1</span>]</span><br></pre></td></tr></table></figure><h3 id="一、with-open-file-train-”r”-as-f中的with和as-f是干嘛用的？"><a href="#一、with-open-file-train-”r”-as-f中的with和as-f是干嘛用的？" class="headerlink" title="一、with open(file_train,”r”) as f中的with和as f是干嘛用的？"></a>一、with open(file_train,”r”) as f中的with和as f是干嘛用的？</h3><p>with：自动关闭文件。相当于open()打开而自动使用close()关闭。若只open不close，会导致文件一直被程序占用，其他程序无法打开导致内存泄漏。</p><p>as f：相当于给文件命名</p><h3 id="二、为什么需要list-？"><a href="#二、为什么需要list-？" class="headerlink" title="二、为什么需要list()？"></a>二、为什么需要list()？</h3><p>因为csv.reader()的作用是按照行分隔符（“&#x2F;n”）和列分隔符（“，”）做切割，如</p><p><img src="/images/1.png" alt="image-20251230181802459"></p><p>但这个功能是一个迭代器，只能单向遍历一次，不能索引，故需要转换成list以便于直接访问</p><h2 id="第二，特征筛选（可选）"><a href="#第二，特征筛选（可选）" class="headerlink" title="第二，特征筛选（可选）"></a>第二，特征筛选（可选）</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">get_feature_importance</span>(<span class="params">feature_data, label_data, k =<span class="number">4</span>,column = <span class="literal">None</span></span>):</span><br><span class="line">    model = SelectKBest(chi2, k=k)      <span class="comment">#定义一个选择k个最佳特征的函数</span></span><br><span class="line">    feature_data = np.array(feature_data, dtype=np.float64)</span><br><span class="line">    X_new = model.fit_transform(feature_data, label_data)   <span class="comment">#用这个函数选择k个最佳特征</span></span><br><span class="line">    <span class="comment">#feature_data是特征数据，label_data是标签数据，该函数可以选择出k个特征</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;x_new&#x27;</span>, X_new)</span><br><span class="line">    scores = model.scores_                <span class="comment"># scores即每一列与结果的相关性</span></span><br><span class="line">    <span class="comment"># 按重要性排序，选出最重要的 k 个</span></span><br><span class="line">    indices = np.argsort(scores)[::-<span class="number">1</span>]        <span class="comment">#[::-1]表示反转一个列表或者矩阵。</span></span><br><span class="line">    <span class="comment"># argsort这个函数， 可以矩阵排序后的下标。 比如 indices[0]表示的是，scores中最小值的下标。</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> column:                            <span class="comment"># 如果需要打印选中的列</span></span><br><span class="line">        k_best_features = [column[i+<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> indices[<span class="number">0</span>:k].tolist()]         <span class="comment"># 选中这些列 打印</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;k best features are: &#x27;</span>,k_best_features)</span><br><span class="line">    <span class="keyword">return</span> X_new, indices[<span class="number">0</span>:k]                  <span class="comment"># 返回选中列的特征和他们的下标。</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> all_feature:</span><br><span class="line">                col_indices = np.array([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">93</span>)])</span><br><span class="line">                col_indices = col_indices.tolist()</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                _,col_indices = get_feature_importance(x,y,feature_dim,column)</span><br><span class="line">                col_indices = col_indices.tolist()</span><br><span class="line">                </span><br><span class="line">            csv_data = np.array(csv_data[<span class="number">1</span>:])[:, <span class="number">1</span>:].astype(<span class="built_in">float</span>)</span><br></pre></td></tr></table></figure><h3 id="feature-data-label-data分别是什么参数？"><a href="#feature-data-label-data分别是什么参数？" class="headerlink" title="feature_data, label_data分别是什么参数？"></a>feature_data, label_data分别是什么参数？</h3><p>是x和y，即用来训练的数据部分和用来训练的已知的训练结果标签</p><h2 id="第三，拆分数据集（只处理了y）"><a href="#第三，拆分数据集（只处理了y）" class="headerlink" title="第三，拆分数据集（只处理了y）"></a>第三，拆分数据集（只处理了y）</h2><p>训练集逢五取四，验证集逢五取一，测试集全取。</p><p>y是训练、验证的标签，由于是这个类里面其他函数也要用的部分，故用self.y变成全局的。由于要后续使用，故转换成tensor</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> mode == <span class="string">&quot;train&quot;</span>:</span><br><span class="line">    indices = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(csv_data)) <span class="keyword">if</span> i % <span class="number">5</span> !=<span class="number">0</span>]</span><br><span class="line">    <span class="variable language_">self</span>.y = torch.tensor(csv_data[indices,-<span class="number">1</span>])</span><br><span class="line"><span class="keyword">elif</span> mode == <span class="string">&quot;val&quot;</span>:</span><br><span class="line">    indices = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(csv_data)) <span class="keyword">if</span> i % <span class="number">5</span> ==<span class="number">0</span>]</span><br><span class="line">    <span class="variable language_">self</span>.y = torch.tensor(csv_data[indices,-<span class="number">1</span>])</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    indices = [i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(csv_data))]</span><br></pre></td></tr></table></figure><h2 id="第四，特征提取（处理x），归一化，校验"><a href="#第四，特征提取（处理x），归一化，校验" class="headerlink" title="第四，特征提取（处理x），归一化，校验"></a>第四，特征提取（处理x），归一化，校验</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">data = torch.tensor(csv_data[indices,:])<span class="comment"># 只取选中模式（训练、验证、测试）下的行</span></span><br><span class="line">data = torch.tensor(csv_data[:,col_indices])<span class="comment">#只取挑出来的列</span></span><br><span class="line"><span class="variable language_">self</span>.data = data</span><br><span class="line"><span class="variable language_">self</span>.mode = mode</span><br><span class="line"><span class="variable language_">self</span>.data = (<span class="variable language_">self</span>.data - <span class="variable language_">self</span>.data.mean(dim=<span class="number">0</span>,keepdim=<span class="literal">True</span>)) / <span class="variable language_">self</span>.data.std(dim=<span class="number">0</span>,keepdim=<span class="literal">True</span>)<span class="comment"># 归一化</span></span><br><span class="line"><span class="keyword">assert</span> feature_dim == <span class="variable language_">self</span>.data.shape[<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Finished reading the &#123;&#125; set of COVID19 Dataset (&#123;&#125; samples found, each dim = &#123;&#125;)&#x27;</span>.<span class="built_in">format</span>(mode, <span class="built_in">len</span>(<span class="variable language_">self</span>.data), feature_dim))  <span class="comment"># 打印读了多少数据</span></span><br></pre></td></tr></table></figure><h3 id="归一化？"><a href="#归一化？" class="headerlink" title="归一化？"></a>归一化？</h3><p><strong>标准化（Z-Score）公式</strong></p><p>这段代码实现的归一化遵循严格的数学公式：</p><p>$X_{norm} &#x3D; \frac{X - \mu}{\sigma}$</p><p>其中：</p><ul><li><em>X</em>：原始特征数据（这里是 <code>self.data</code> 张量中的某个特征的所有样本值）</li><li><em>μ</em>（mu）：该特征的<strong>均值</strong>（所有样本在该特征上的平均值，对应代码中 <code>self.data.mean(dim=0, keepdim=True)</code>）</li><li><em>σ</em>（sigma）：该特征的<strong>标准差</strong>（所有样本在该特征上的离散程度，对应代码中 <code>self.data.std(dim=0, keepdim=True)</code>）</li><li><strong>X</strong>norm：标准化后的特征数据，最终满足「均值为 0，标准差为 1」的分布特性</li></ul><h3 id="dim-0-为啥是对列归一化？"><a href="#dim-0-为啥是对列归一化？" class="headerlink" title="dim&#x3D;0,为啥是对列归一化？"></a>dim&#x3D;0,为啥是对列归一化？</h3><p>dim为0按列，为1按行</p><p><strong>归一化的本质是对「特征」做预处理，每一列对应一个独立特征，每一行对应一个样本的特征集合，按列归一化是符合深度学习 &#x2F; 机器学习逻辑的必然选择，按行归一化无实际业务和模型意义</strong>。</p><h3 id="keepdim-True的作用？"><a href="#keepdim-True的作用？" class="headerlink" title="keepdim&#x3D;True的作用？"></a><strong>keepdim&#x3D;True</strong>的作用？</h3><p>keepdim默认为false，即计算过程中压缩计算维度为1维，会使维度不匹配而报错。令其为true可使张量维度保持不变。</p><h3 id="assert在这里的作用？"><a href="#assert在这里的作用？" class="headerlink" title="assert在这里的作用？"></a>assert在这里的作用？</h3><p>保险措施，<strong>强制校验特征维度</strong>，提前暴露特征筛选错误，避免后续模型训练时出现维度不匹配异常。</p><p><code>self.data.shape[1]</code>：获取归一化后特征张量的列数（即实际筛选得到的特征维度），<code>self.data.shape[0]</code> 是样本数；<code>feature_dim</code>：用户指定的目标特征维度（k，即要保留的重要特征数）；</p><p>assert 断言的作用：<br>若 feature_dim &#x3D;&#x3D; self.data.shape[1]（维度一致），程序正常继续执行；<br>若维度不一致（如 col_indices 筛选了 3 个特征，但 feature_dim&#x3D;5），立即抛出 AssertionError 异常，终止程序运行；</p><h2 id="第五，getitem和len"><a href="#第五，getitem和len" class="headerlink" title="第五，getitem和len"></a>第五，getitem和len</h2><p><strong>支持通过下标（索引）<code>item</code> 读取数据集对应位置的数据</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, item</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="variable language_">self</span>.mode == <span class="string">&quot;test&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[item].<span class="built_in">float</span>()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.data[item].<span class="built_in">float</span>(),<span class="variable language_">self</span>.y[item].<span class="built_in">float</span>()</span><br><span class="line"> <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.data)                 <span class="comment"># 返回数据长度。</span></span><br></pre></td></tr></table></figure><h2 id="另外，类的书写和函数不一样，类会调用括号里面的基类，如Dataset-nn-Module"><a href="#另外，类的书写和函数不一样，类会调用括号里面的基类，如Dataset-nn-Module" class="headerlink" title="另外，类的书写和函数不一样，类会调用括号里面的基类，如Dataset,nn.Module"></a>另外，类的书写和函数不一样，类会调用括号里面的基类，如Dataset,nn.Module</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">myNet</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,inDim</span>):</span><br><span class="line">        <span class="built_in">super</span>(myNet,<span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.fc1 = nn.Linear(inDim, <span class="number">64</span>)              <span class="comment"># 全连接</span></span><br><span class="line">        <span class="variable language_">self</span>.relu = nn.ReLU()                      <span class="comment"># 激活函数</span></span><br><span class="line">        <span class="variable language_">self</span>.fc2 = nn.Linear(<span class="number">64</span>,<span class="number">1</span>)                  <span class="comment"># 全连接</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">covidDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, path, mode=<span class="string">&quot;train&quot;</span>, feature_dim=<span class="number">5</span>, all_feature=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(path,<span class="string">&#x27;r&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            csv_data = <span class="built_in">list</span>(csv.reader(f))</span><br><span class="line">            column = csv_data[<span class="number">0</span>]</span><br><span class="line">            x = np.array(csv_data)[<span class="number">1</span>:,<span class="number">1</span>:-<span class="number">1</span>]</span><br><span class="line">            y = np.array(csv_data)[<span class="number">1</span>:,-<span class="number">1</span>]</span><br><span class="line">   </span><br></pre></td></tr></table></figure><h3 id="为啥Module要super，Dataset却不用？"><a href="#为啥Module要super，Dataset却不用？" class="headerlink" title="为啥Module要super，Dataset却不用？"></a>为啥Module要super，Dataset却不用？</h3><p>因为Dataset并没有用到什么高级的方法，都是我自己定义的。</p><p>而myNet需要用到Module基类里的高级方法，如果不super，模型就识别不出fc1、relu、fc2有何意味。</p><p>super的用法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">super</span>().__init__()</span><br></pre></td></tr></table></figure><h2 id="第六，前向传播"><a href="#第六，前向传播" class="headerlink" title="第六，前向传播"></a>第六，前向传播</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">    x = <span class="variable language_">self</span>.fc1(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.relu(x)</span><br><span class="line">    x = <span class="variable language_">self</span>.fc2(x)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(x.size()) &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> x.squeeze(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h3 id="squeeze起什么作用？不是已经被压缩到一维了吗？"><a href="#squeeze起什么作用？不是已经被压缩到一维了吗？" class="headerlink" title="squeeze起什么作用？不是已经被压缩到一维了吗？"></a>squeeze起什么作用？不是已经被压缩到一维了吗？</h3><p>虽然self.fc2 &#x3D; nn.Linear(64,1)确实把维度降到了一，但实际上还是形状为(样本数量，1)的二维张量。这让结果看起来是一个n*1的矩阵，而squeeze的作用就是把结果转换为大小为n的一维数组</p><h2 id="第七，模型训练验证中的训练部分"><a href="#第七，模型训练验证中的训练部分" class="headerlink" title="第七，模型训练验证中的训练部分"></a>第七，模型训练验证中的训练部分</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train_val</span>(<span class="params">model, trainloader, valloader, optimizer, loss ,epoch,device,save_</span>)</span><br><span class="line">    model =model.to(device)</span><br><span class="line">    plt_train_loss = []</span><br><span class="line">    plt_val_loss = []</span><br><span class="line">    val_rel = []</span><br><span class="line">    min_val_loss = <span class="number">1000000</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">        strat_time = time.time()</span><br><span class="line">        model.train()</span><br><span class="line">        train_loss = <span class="number">0.0</span></span><br><span class="line">        val_loss = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> trainloader:</span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            x, target = data[<span class="number">0</span>].to(device),data[<span class="number">1</span>].to(device)</span><br><span class="line">            pred = model(x)</span><br><span class="line">            bat_loss = loss(pred,target,model)</span><br><span class="line">            optimizer.step()</span><br><span class="line">            train_loss += bat_loss.detach().cpu().item()</span><br><span class="line">     plt_train_loss. append(train_loss/trainloader.dataset.__len__())</span><br></pre></td></tr></table></figure><h3 id="这些参数分别啥意思？"><a href="#这些参数分别啥意思？" class="headerlink" title="这些参数分别啥意思？"></a>这些参数分别啥意思？</h3><p>model模型, trainloader训练集数据, valloader验证集数据, </p><p>optimizer动态调整梯度的调节方向和调节速率，通过学习率控制,</p><p> loss 损失,epoch训练轮次,device设备,save_保存到哪里</p><h3 id="model-model-to-device-什么作用？"><a href="#model-model-to-device-什么作用？" class="headerlink" title="model &#x3D;model.to(device)什么作用？"></a>model &#x3D;model.to(device)什么作用？</h3><p>把模型搬到同一个设备上，保证在同一块CPU或者GPU上运行</p><h3 id="optimizer-zero-grad-？"><a href="#optimizer-zero-grad-？" class="headerlink" title="optimizer.zero_grad()？"></a>optimizer.zero_grad()？</h3><p>清空每一批数据的梯度，防止被传到下一批中影响结果，导致参数更新错误</p><p>梯度可以类比学生错在哪里的错题分析。如果新的一轮不清除上一轮的错题分析，会影响新一轮批次的梯度。</p><h3 id="pred-model-x-？"><a href="#pred-model-x-？" class="headerlink" title="pred &#x3D; model(x)？"></a>pred &#x3D; model(x)？</h3><p>model(x) 会自动触发 model.forward(x) 方法，这是 PyTorch 帮我们封装好的便捷操作</p><p>把输入数据 x 传入模型，让模型按照你定义的 forward 方法执行计算，最终返回预测结果 pred（存到 pred 变量里）。</p><h3 id="bat-loss-loss-pred-target-model"><a href="#bat-loss-loss-pred-target-model" class="headerlink" title="bat_loss &#x3D; loss(pred,target,model)"></a>bat_loss &#x3D; loss(pred,target,model)</h3><p>即batch_loss，loss()是函数传入的损失函数，且得到的结果是一个一维张量，想要使用他还要转成数字</p><h3 id="trainloader-dataset-len"><a href="#trainloader-dataset-len" class="headerlink" title="trainloader.dataset.len()"></a>trainloader.dataset.<strong>len</strong>()</h3><p>记录每一轮训练的平均损失值，为后续绘制损失曲线做准备。训练集的 “总题数”（总样本数）</p><h2 id="第八，模型训练验证中的验证部分"><a href="#第八，模型训练验证中的验证部分" class="headerlink" title="第八，模型训练验证中的验证部分"></a>第八，模型训练验证中的验证部分</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="keyword">for</span> data <span class="keyword">in</span> valloader:</span><br><span class="line">                val_x, val_target = data[<span class="number">0</span>].to(device), data[<span class="number">1</span>].to(device)</span><br><span class="line">                val_pred = model(val_x)</span><br><span class="line">                val_bat_loss = loss(val_pred,val_target,model)</span><br><span class="line">                val_loss += val_bat_loss.detach().cpu().item()</span><br><span class="line">        <span class="keyword">if</span> val_loss &lt; min_val_loss:</span><br><span class="line">            torch.save(model, save_)</span><br><span class="line">        plt_val_loss.append(val_loss/valloader.dataset.__len__())</span><br></pre></td></tr></table></figure><h3 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h3><p>第一、model切换训练模式是model.train(),但切换验证模式是eval()而不是val()</p><p>第二、若得到了更小的损失，torch.save(model, save_)保存的是模型</p><h2 id="第九，输出模型结果"><a href="#第九，输出模型结果" class="headerlink" title="第九，输出模型结果"></a>第九，输出模型结果</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate</span>(<span class="params">model_path, testset,rel_path, device</span>):</span><br><span class="line">    model = torch.load(model_path).to(device)</span><br><span class="line">    testloader = DataLoader(testset,batch_size=<span class="number">1</span>,shuffle=<span class="literal">False</span>)</span><br><span class="line">    val_rel = []</span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">        x = data.to(device)</span><br><span class="line">        pred = model(x)</span><br><span class="line">        val_rel.append(pred.item())</span><br><span class="line">    <span class="built_in">print</span>(val_rel)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(rel_path,<span class="string">&quot;w&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        csv_data = csv.writer(f)</span><br><span class="line">        csv_data.writerow([<span class="string">&#x27;id&#x27;</span>,<span class="string">&#x27;tested_positive&#x27;</span>])</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(testset)):</span><br><span class="line">            csv_data.writerow([<span class="built_in">str</span>(i),<span class="built_in">str</span>(val_rel[i])])</span><br></pre></td></tr></table></figure><h3 id="shuffle有什么作用？"><a href="#shuffle有什么作用？" class="headerlink" title="shuffle有什么作用？"></a>shuffle有什么作用？</h3><p>控制是否对数据集的样本顺序进行打乱，可以改变样本被模型读取的顺序。</p><p>在测试时需要打乱取True，若样本是按照类别排序或者有时间顺序，如果不打乱，模型会学习到虚假的特征而忽略了真正的特征，导致模型过拟合。比如你的训练集是按类别排的（先全是猫，再全是狗），不洗牌的话，模型会连续学很久猫，再连续学很久狗，容易学歪（比如误以为 “前面的都是猫，后面的都是狗”，而不是学猫和狗的本质特征）。</p><p>在验证和测试时不需要打乱取False，首先保证 id 和预测结果对得上：你要把样本索引i和预测结果一一写到 CSV 里（比如第 0 个样本对应第 0 个预测值），要是洗牌了，样本顺序乱了，你记录的预测结果就和原始样本对不上了，相当于 “张冠李戴”，后续根本没法追溯哪个样本对应哪个结果。其次，测试阶段洗牌没用，纯添乱：洗牌是为了帮模型更好训练，测试阶段模型已经定型了，不用再学东西，洗牌除了搞乱顺序，一点好处都没有。</p><h2 id="第十，动手做数据准备"><a href="#第十，动手做数据准备" class="headerlink" title="第十，动手做数据准备"></a>第十，动手做数据准备</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">all_col = <span class="literal">False</span>              <span class="comment">#是否使用所有的列</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>       <span class="comment">#选择使用cpu还是gpu计算。</span></span><br><span class="line"><span class="built_in">print</span>(device)</span><br><span class="line">train_path = <span class="string">&#x27;covid.train.csv&#x27;</span>                     <span class="comment"># 训练数据路径</span></span><br><span class="line">test_path = <span class="string">&#x27;covid.test.csv&#x27;</span>              <span class="comment"># 测试数据路径</span></span><br><span class="line"></span><br><span class="line">file = pd.read_csv(train_path)</span><br><span class="line">file.head()                    <span class="comment"># 用pandas 看看数据长啥样</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> all_col == <span class="literal">True</span>:</span><br><span class="line">    feature_dim = <span class="number">93</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    feature_dim = <span class="number">6</span>              <span class="comment">#是否使用所有的列</span></span><br><span class="line"></span><br><span class="line">trainset = covidDataset(train_path,<span class="string">&#x27;train&#x27;</span>,feature_dim=feature_dim, all_feature=all_col)</span><br><span class="line">valset = covidDataset(train_path,<span class="string">&#x27;val&#x27;</span>,feature_dim=feature_dim, all_feature=all_col)</span><br><span class="line">testset = covidDataset(test_path,<span class="string">&#x27;test&#x27;</span>,feature_dim=feature_dim, all_feature=all_col)   <span class="comment">#读取训练， 验证，测试数据</span></span><br></pre></td></tr></table></figure><p>简单说，这段代码是<strong>机器学习项目的 “数据准备第一步”</strong>：先设置一些基础配置（是否用全量特征、选择 CPU&#x2F;GPU），再读取 CSV 格式的原始数据，最后通过自定义数据集类把训练集、验证集、测试集都加载好，为后续模型训练和预测做准备。</p><h2 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mseLoss</span>(<span class="params">pred,target,model</span>):</span><br><span class="line">    loss = nn.MSELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">    regularization_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> model.parameters():</span><br><span class="line">        regularization_loss += torch.<span class="built_in">sum</span>(i ** <span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> loss(pred,target) + regularization_loss * <span class="number">0.00075</span></span><br><span class="line">loss</span><br><span class="line">loss =mseLoss</span><br></pre></td></tr></table></figure><h3 id="loss-nn-MSELoss-reduction-’mean’"><a href="#loss-nn-MSELoss-reduction-’mean’" class="headerlink" title="loss &#x3D; nn.MSELoss(reduction&#x3D;’mean’)"></a>loss &#x3D; nn.MSELoss(reduction&#x3D;’mean’)</h3><p>MSELoss是Mean Squared Error Loss的缩写，即均方误差损失，计算“模型预测值”和“真实标签值”之间的平均均方误差。mean是指误差计算的最终拟合方式，计算每个样本的均方误差，再计算它们的平均值</p><h3 id="regularization-loss-0-和-regularization-loss-torch-sum-i-2"><a href="#regularization-loss-0-和-regularization-loss-torch-sum-i-2" class="headerlink" title="regularization_loss &#x3D; 0 和 regularization_loss +&#x3D; torch.sum(i ** 2)"></a>regularization_loss &#x3D; 0 和 regularization_loss +&#x3D; torch.sum(i ** 2)</h3><p>regularization_loss &#x3D; 0这个是正则项，初始为零，后续累加模型所有参数的惩罚值，通过给模型加惩罚项，可以避免模型过度拟合训练数据。</p><p> regularization_loss +&#x3D; torch.sum(i ** 2)是L2正则化。防止模型通过把参数拉大的方式只取得在训练集上的优秀表现导致在测试集上表现很差。加了 L2 正则化后，参数太大会被惩罚，模型只能学习数据的通用特征，而不是训练数据的噪声，从而提升泛化能力。</p><h2 id="第十一，动手"><a href="#第十一，动手" class="headerlink" title="第十一，动手"></a>第十一，动手</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">config = &#123;</span><br><span class="line">    <span class="string">&#x27;epochs&#x27;</span>:<span class="number">50</span>,</span><br><span class="line">    <span class="string">&#x27;batch_size&#x27;</span>:<span class="number">256</span>,</span><br><span class="line">    <span class="string">&#x27;optimizer&#x27;</span>:<span class="string">&#x27;SGD&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;optim_hparas&#x27;</span>:&#123;</span><br><span class="line">        <span class="string">&#x27;lr&#x27;</span>:<span class="number">0.0001</span>,</span><br><span class="line">        <span class="string">&#x27;momentum&#x27;</span>:<span class="number">0.9</span></span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&#x27;early_stop&#x27;</span>:<span class="number">200</span>,</span><br><span class="line">    <span class="string">&#x27;save_path&#x27;</span>:<span class="string">&#x27;model_save/model_path&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">model = myNet(feature_dim).to(device)</span><br><span class="line">optimizer = optim.SGD(model.parameters(),lr=<span class="number">0.001</span>,momentum=<span class="number">0.9</span>)</span><br><span class="line">trainloader = DataLoader(train_set,config[<span class="string">&#x27;batch_size&#x27;</span>],shuffle=<span class="literal">True</span>)</span><br><span class="line">valloader = DataLoader(val_set,config[<span class="string">&#x27;batch_size&#x27;</span>],shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">train_val(model,trainloader,valloader,optimizer,loss,config[<span class="string">&#x27;epochs&#x27;</span>],device,config[<span class="string">&#x27;save_path&#x27;</span>])</span><br><span class="line">evaluate(config[<span class="string">&#x27;save_path&#x27;</span>],test_set,<span class="string">&#x27;pred.csv&#x27;</span>,device)</span><br></pre></td></tr></table></figure><h3 id="参数含义？"><a href="#参数含义？" class="headerlink" title="参数含义？"></a>参数含义？</h3><p>SGD：随机梯度下降</p><p>optim_hparas：SGD的参数。lr是学习率，即模型更新的步长，太小导致收敛变慢，太大导致不收敛。momentum是动量，靠 “惯性冲力” 强行冲出局部最优的束缚，避免在局部最优解附近来回震荡。类比模拟退火靠 “一定概率接受差解” 的<strong>随机</strong>性，跳出局部最优的陷阱。</p><p>early_stop：早停阈值，如果模型连续n轮验证集性能都没提升，就提前停止训练，避免无效训练浪费时间</p><h3 id="optimizer-optim-SGD-model-parameters-lr-0-001-momentum-0-9"><a href="#optimizer-optim-SGD-model-parameters-lr-0-001-momentum-0-9" class="headerlink" title="optimizer &#x3D; optim.SGD(model.parameters(),lr&#x3D;0.001,momentum&#x3D;0.9)"></a>optimizer &#x3D; optim.SGD(model.parameters(),lr&#x3D;0.001,momentum&#x3D;0.9)</h3><p> “给模型分配一个‘助手’，这个助手会用 SGD 策略，按照指定的步长和惯性，帮模型调整参数，让模型的预测误差越来越小”</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 回归 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
